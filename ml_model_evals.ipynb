{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating ML Models as Agent Tools\n",
    "\n",
    "In this notebook, we'll demonstrate how to evaluate traditional ML models when used as tools within LangGraph agents. We'll build a clinical trial feedback analysis agent and show multiple evaluation approaches:\n",
    "\n",
    "1. **End-to-End Evaluation** - Does the agent produce correct final responses?\n",
    "2. **Single Step Evaluation** - Did the agent correctly select the right tool?\n",
    "3. **Trajectory Evaluation** - Did the agent take the expected path of tool calls?\n",
    "4. **ML Model Metrics** - Precision, Recall, F1, Accuracy, Calibration on the model itself\n",
    "\n",
    "This pattern applies to any ML model or tool called by an agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations: Key Concepts\n",
    "\n",
    "**Evaluations** are a quantitative way to measure performance of agents, which is important because LLMs don't always behave predictably — small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\n",
    "\n",
    "Evaluations are made up of three components:\n",
    "\n",
    "1. A **dataset** with test inputs and expected outputs.\n",
    "2. An **application or target function** that defines what you are evaluating, taking in inputs and returning the application output\n",
    "3. **Evaluators** that score your target function's outputs.\n",
    "\n",
    "![Evaluation](images/evals-conceptual.png)\n",
    "\n",
    "In this notebook, we'll cover:\n",
    "\n",
    "1. **End-to-End (Final Response)**: Evaluate the agent's overall output quality\n",
    "2. **Single Step**: Evaluate whether the agent selects the appropriate tool\n",
    "3. **Trajectory**: Evaluate whether the agent took the expected path of tool calls\n",
    "4. **ML Model Metrics**: Evaluate the ML tool directly with precision, recall, F1, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "First, let's install the required packages. We need:\n",
    "- `transformers` and `torch` for the HuggingFace sentiment model\n",
    "- `langchain` and `langgraph` for building the agent\n",
    "- `langsmith` and `openevals` for evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "**Use Case: Clinical Trial Participant Feedback Analysis**\n",
    "\n",
    "In pharmaceutical research, analyzing sentiment from clinical trial participant feedback helps identify:\n",
    "- Treatment satisfaction and efficacy perceptions\n",
    "- Side effect severity and patient concerns  \n",
    "- Early signals of safety issues\n",
    "- Overall participant experience\n",
    "\n",
    "We'll build an agent that uses sentiment analysis to classify this feedback, then evaluate it thoroughly.\n",
    "\n",
    "Make sure you have the following environment variables set:\n",
    "- `LANGSMITH_API_KEY` - For LangSmith evaluations\n",
    "- `OPENAI_API_KEY` - For the LLM powering the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the LLM for our agent\n",
    "model = init_chat_model(\"openai:gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building the Sentiment Analysis Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create the Sentiment Analysis Tool\n",
    "\n",
    "We'll wrap a HuggingFace sentiment analysis model as a LangChain tool. This model (`distilbert-base-uncased-finetuned-sst-2-english`) classifies text as POSITIVE or NEGATIVE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Initialize the HuggingFace sentiment pipeline\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "@tool\n",
    "def analyze_sentiment(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of the given text.\n",
    "    Returns the sentiment label (POSITIVE/NEGATIVE) and confidence score.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze for sentiment\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'label' (POSITIVE/NEGATIVE) and 'confidence' (0-1)\n",
    "    \"\"\"\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return {\"label\": result[\"label\"], \"confidence\": round(result[\"score\"], 4)}\n",
    "\n",
    "# Test the tool\n",
    "print(\"Testing sentiment tool:\")\n",
    "print(analyze_sentiment.invoke(\"I love this product, it's amazing!\"))\n",
    "print(analyze_sentiment.invoke(\"This is terrible, I hate it.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a tool to flag for review\n",
    "\n",
    "This tool complements the sentiment analysis by adding an **actionable triage layer** - while sentiment tells us *how* a participant feels, the flag tool identifies feedback that needs *immediate human review*, regardless of overall sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool  \n",
    "def flag_for_review(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if feedback should be flagged for urgent medical team review.\n",
    "    Use this when feedback mentions serious concerns.\n",
    "    \n",
    "    Args:\n",
    "        text: The feedback text to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'requires_review' (bool) and 'reason'\n",
    "    \"\"\"\n",
    "    urgent_keywords = [\"withdraw\", \"discontinue\", \"severe\", \"emergency\", \"hospitalized\", \"worse\"]\n",
    "    has_urgent_keyword = any(kw in text.lower() for kw in urgent_keywords)\n",
    "    \n",
    "    if has_urgent_keyword:\n",
    "        return {\"requires_review\": True, \"reason\": \"Contains urgent keywords - recommend medical team review\"}\n",
    "    return {\"requires_review\": False, \"reason\": \"No urgent flags detected\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create the Agent\n",
    "\n",
    "Now let's create a simple agent using LangChain's `create_agent()` that can use our sentiment analysis tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# System prompt for our clinical trial feedback sentiment agent\n",
    "sentiment_agent_prompt = \"\"\"\n",
    "You are a clinical trial feedback analyst that helps pharmaceutical researchers \n",
    "understand participant sentiment and identify feedback requiring urgent attention.\n",
    "\n",
    "You have access to two tools:\n",
    "1. analyze_sentiment - Use this to classify feedback as positive or negative\n",
    "2. flag_for_review - Use this to check if feedback should be flagged for urgent medical team review\n",
    "\n",
    "When given participant feedback:\n",
    "- ALWAYS use analyze_sentiment to determine the sentiment\n",
    "- ONLY use flag_for_review if the sentiment response from analyze_sentiment is negative to check for urgent concerns that need medical team attention\n",
    "- After using the tools, provide a comprehensive summary including:\n",
    "  - The sentiment classification and what it indicates\n",
    "  - Whether the feedback is flagged for review and why\n",
    "\n",
    "If the user asks something unrelated to clinical trial feedback analysis, politely \n",
    "explain that you specialize in analyzing participant feedback from clinical studies.\n",
    "\"\"\"\n",
    "\n",
    "# Create checkpointer for conversation memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Create the agent\n",
    "sentiment_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[analyze_sentiment, flag_for_review],\n",
    "    name=\"clinical_feedback_agent\",\n",
    "    system_prompt=sentiment_agent_prompt,\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "sentiment_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Test the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langsmith import uuid7\n",
    "\n",
    "# Test with clinical trial participant feedback\n",
    "config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "result = sentiment_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Analyze this participant feedback: 'This treatment has been life-changing! My symptoms have completely disappeared and I feel better than I have in years. I'm so grateful I enrolled in this trial.'\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an unrelated query (should call flag_for_review)\n",
    "config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "result = sentiment_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"The treatment made my symptoms worse and I'm considering discontinuing.\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: End-to-End Evaluation (Final Response)\n",
    "\n",
    "Let's evaluate the agent's complete responses. This treats the agent as a black box and evaluates whether it produces correct, helpful responses.\n",
    "\n",
    "- Input: User query\n",
    "- Output: Agent's final response\n",
    "\n",
    "![final-response](images/final-response.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create the Dataset\n",
    "\n",
    "We'll create a dataset with labeled reference outputs. In production, you'd use a curated test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "e2e_examples = [\n",
    "    # Positive feedback - should only call sentiment, no flag needed\n",
    "    {\n",
    "        \"query\": \"The treatment has dramatically reduced my symptoms. I finally feel like myself again.\",\n",
    "        \"expected_response\": \"Positive sentiment indicating strong treatment efficacy and improved quality of life. No flag for review needed.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The study team was excellent and the medication worked better than anything I've tried before.\",\n",
    "        \"expected_response\": \"Positive sentiment about both treatment efficacy and study experience. No flag for review needed.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"No side effects and my pain levels have dropped significantly since week 2.\",\n",
    "        \"expected_response\": \"Positive sentiment indicating good tolerability and measurable improvement. No flag for review needed.\"\n",
    "    },\n",
    "    \n",
    "    # Negative feedback WITH urgent keywords - should call both tools, flag=True\n",
    "    {\n",
    "        \"query\": \"I had to withdraw due to severe nausea and persistent headaches.\",\n",
    "        \"expected_response\": \"Negative sentiment indicating tolerability issues. Flagged for medical team review due to withdrawal and severe symptoms.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The side effects were unbearable. I was hospitalized last week.\",\n",
    "        \"expected_response\": \"Negative sentiment with serious safety concern. Flagged for urgent medical team review due to hospitalization.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The treatment made my symptoms worse. I want to discontinue immediately.\",\n",
    "        \"expected_response\": \"Negative sentiment indicating worsening condition. Flagged for review due to intent to discontinue and worsening symptoms.\"\n",
    "    },\n",
    "    \n",
    "    # Negative feedback WITHOUT urgent keywords - should call both tools, flag=False  \n",
    "    {\n",
    "        \"query\": \"No improvement after 8 weeks. Very disappointed with the results.\",\n",
    "        \"expected_response\": \"Negative sentiment indicating lack of efficacy. Checked for urgent flags - none detected.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The medication didn't help and the clinic visits were inconvenient.\",\n",
    "        \"expected_response\": \"Negative sentiment about efficacy and study experience. No urgent flags requiring immediate review.\"\n",
    "    },\n",
    "    \n",
    "    # Off-topic - should not use any tools\n",
    "    {\n",
    "        \"query\": \"What's the recommended dosage for ibuprofen?\",\n",
    "        \"expected_response\": \"Politely decline - specializes in clinical trial feedback analysis only.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "e2e_dataset_name = \"ML Model Evals: Clinical Trial E2E v3\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=e2e_dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=e2e_dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"query\": ex[\"query\"]} for ex in e2e_examples],\n",
    "        outputs=[{\"response\": ex[\"expected_response\"]} for ex in e2e_examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    print(f\"Created dataset: {e2e_dataset_name}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {e2e_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define the Target Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "async def run_agent_e2e(inputs: dict) -> dict:\n",
    "    \"\"\"Run the full agent and return the final response.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "    \n",
    "    result = await sentiment_agent.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=inputs[\"query\"])]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Just return the final response\n",
    "    return {\"response\": result[\"messages\"][-1].content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define the Evaluator\n",
    "\n",
    "We'll use a prebuilt **correctness evaluator** from the LangChain [openevals](https://github.com/langchain-ai/openevals) library. OpenEvals provides ready-to-use LLM-as-judge evaluators with well-tested prompts for common evaluation criteria like correctness, conciseness, and relevance.\n",
    "\n",
    "The `CORRECTNESS_PROMPT` compares the agent's response against a reference output to determine if it conveys the same meaning and key information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openevals.llm import create_async_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "\n",
    "# LLM-as-judge for response correctness\n",
    "correctness_evaluator = create_async_llm_as_judge(\n",
    "    prompt=CORRECTNESS_PROMPT,\n",
    "    feedback_key=\"correctness\",\n",
    "    judge=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Run the Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    run_agent_e2e,\n",
    "    data=e2e_dataset_name,\n",
    "    evaluators=[correctness_evaluator],\n",
    "    experiment_prefix=\"sentiment-e2e\",\n",
    "    max_concurrency=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Single Step Evaluation\n",
    "\n",
    "Now let's evaluate whether the agent correctly decides on the next step. In this case, we will determine whether or not it correctly selected when to use the flagging tool. \n",
    "\n",
    "- Input: User query\n",
    "- Output: Whether the correct tool was selected (or no tool for no flagging words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_examples = [\n",
    "    # Negative feedback - SHOULD call flag_for_review after sentiment\n",
    "    {\n",
    "        \"query\": \"I had to withdraw due to severe nausea and persistent headaches.\",\n",
    "        \"should_call_flag_tool\": True\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The side effects were unbearable. I was hospitalized last week.\",\n",
    "        \"should_call_flag_tool\": True\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The treatment made my symptoms worse. I want to discontinue immediately.\",\n",
    "        \"should_call_flag_tool\": True\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"No improvement after 8 weeks. Very disappointed with the results.\",\n",
    "        \"should_call_flag_tool\": True\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The medication didn't help and the clinic visits were inconvenient.\",\n",
    "        \"should_call_flag_tool\": True\n",
    "    },\n",
    "    \n",
    "    # Positive feedback - should NOT call flag_for_review\n",
    "    {\n",
    "        \"query\": \"The treatment has dramatically reduced my symptoms. I finally feel like myself again.\",\n",
    "        \"should_call_flag_tool\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The study team was excellent and the medication worked better than anything I've tried before.\",\n",
    "        \"should_call_flag_tool\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"No side effects and my pain levels have dropped significantly since week 2.\",\n",
    "        \"should_call_flag_tool\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I'm grateful I enrolled. This has been life-changing!\",\n",
    "        \"should_call_flag_tool\": False\n",
    "    },\n",
    "    \n",
    "    # Off-topic - should NOT call flag_for_review (no tools at all)\n",
    "    {\n",
    "        \"query\": \"What's the recommended dosage for ibuprofen?\",\n",
    "        \"should_call_flag_tool\": False\n",
    "    },\n",
    "]\n",
    "\n",
    "single_step_dataset_name = \"ML Model Evals: Clinical Trial Single Step - Flag Tool\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=single_step_dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=single_step_dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"query\": ex[\"query\"]} for ex in single_step_examples],\n",
    "        outputs=[{\"should_call_flag_tool\": ex[\"should_call_flag_tool\"]} for ex in single_step_examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    print(f\"Created dataset: {single_step_dataset_name}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {single_step_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the Target Function\n",
    "\n",
    "We'll run the agent and check if it called the sentiment tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent_single_step(inputs: dict) -> dict:\n",
    "    \"\"\"Run agent and check if flag_for_review tool was called.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "    \n",
    "    result = await sentiment_agent.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=inputs[\"query\"])]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Check if flag_for_review tool was called\n",
    "    flag_tool_called = False\n",
    "    for message in result[\"messages\"]:\n",
    "        if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "            for tc in message.tool_calls:\n",
    "                if tc.get(\"name\") == \"flag_for_review\":\n",
    "                    flag_tool_called = True\n",
    "                    break\n",
    "    \n",
    "    return {\"flag_tool_called\": flag_tool_called}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_tool_selection_evaluator(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Check if the agent correctly decided whether to call the flag tool.\"\"\"\n",
    "    expected = reference_outputs[\"should_call_flag_tool\"]\n",
    "    actual = outputs[\"flag_tool_called\"]\n",
    "    is_correct = expected == actual\n",
    "    return {\"key\": \"correct_flag_selection\", \"score\": 1 if is_correct else 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Run the Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    run_agent_single_step,\n",
    "    data=single_step_dataset_name,\n",
    "    evaluators=[flag_tool_selection_evaluator],\n",
    "    experiment_prefix=\"flag-tool-single-step\",\n",
    "    max_concurrency=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Trajectory Evaluation\n",
    "\n",
    "Evaluating an agent's trajectory involves evaluating all the steps an agent took. The evaluator here is some function over the steps taken. Examples of evaluators include an exact match for each tool name in the sequence or the number of \"incorrect\" steps taken.\n",
    "\n",
    "- Input: User input to the overall agent \n",
    "- Output: A list of tool calls (the trajectory)\n",
    "\n",
    "![trajectory](../langgraph-101/images/trajectory.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create the Dataset\n",
    "\n",
    "Based on our agent's logic:\n",
    "- **Positive feedback** → calls only `analyze_sentiment`\n",
    "- **Negative feedback** → calls `analyze_sentiment` then `flag_for_review`\n",
    "- **Off-topic queries** → calls no tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_examples = [\n",
    "    # Positive feedback - only sentiment analysis\n",
    "    {\n",
    "        \"query\": \"The treatment has dramatically reduced my symptoms. I finally feel like myself again.\",\n",
    "        \"trajectory\": [\"analyze_sentiment\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"No side effects and my pain levels have dropped significantly since week 2.\",\n",
    "        \"trajectory\": [\"analyze_sentiment\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The study team was excellent and the medication worked better than anything I've tried before.\",\n",
    "        \"trajectory\": [\"analyze_sentiment\"]\n",
    "    },\n",
    "    \n",
    "    # Negative feedback - sentiment analysis then flag for review\n",
    "    {\n",
    "        \"query\": \"I had to withdraw due to severe nausea and persistent headaches.\",\n",
    "        \"trajectory\": [\"analyze_sentiment\", \"flag_for_review\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The side effects were unbearable. I was hospitalized last week.\",\n",
    "        \"trajectory\": [\"analyze_sentiment\", \"flag_for_review\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The treatment made my symptoms worse. I want to discontinue immediately.\",\n",
    "        \"trajectory\": [\"analyze_sentiment\", \"flag_for_review\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"No improvement after 8 weeks. Very disappointed with the results.\",\n",
    "        \"trajectory\": [\"analyze_sentiment\", \"flag_for_review\"]\n",
    "    },\n",
    "    \n",
    "    # Off-topic - no tools called\n",
    "    {\n",
    "        \"query\": \"What's the recommended dosage for ibuprofen?\",\n",
    "        \"trajectory\": []\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who is the principal investigator?\",\n",
    "        \"trajectory\": []\n",
    "    },\n",
    "]\n",
    "\n",
    "trajectory_dataset_name = \"ML Model Evals: Clinical Trial Trajectory\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=trajectory_dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=trajectory_dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"query\": ex[\"query\"]} for ex in trajectory_examples],\n",
    "        outputs=[{\"trajectory\": ex[\"trajectory\"]} for ex in trajectory_examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    print(f\"Created dataset: {trajectory_dataset_name}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {trajectory_dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define Helper Function\n",
    "\n",
    "We'll use a helper function to extract and log the names of all the tool calls from the agent's execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "def extract_tool_calls(input: Dict | List[Dict]):\n",
    "    \"\"\"Extract tool calls from the stream format.\"\"\"\n",
    "    tool_calls = []\n",
    "    \n",
    "    # Check for 'tool_call' key directly in input\n",
    "    if isinstance(input, Dict) and \"tool_call\" in input:\n",
    "        tool_call = input[\"tool_call\"]\n",
    "        if isinstance(tool_call, dict) and \"name\" in tool_call:\n",
    "            tool_calls.append(tool_call[\"name\"])\n",
    "    \n",
    "    # Check for messages with tool_calls attribute\n",
    "    elif isinstance(input, Dict) and \"messages\" in input:\n",
    "        for message in input[\"messages\"]:\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                for tc in message.tool_calls:\n",
    "                    if isinstance(tc, dict) and \"name\" in tc:\n",
    "                        tool_calls.append(tc[\"name\"])\n",
    "            elif hasattr(message, 'additional_kwargs') and message.additional_kwargs.get(\"tool_calls\"):\n",
    "                tools = message.additional_kwargs[\"tool_calls\"]\n",
    "                tool_calls.extend([tool[\"function\"][\"name\"] for tool in tools])\n",
    "    \n",
    "    elif isinstance(input, List):\n",
    "        for item in input:\n",
    "            if isinstance(item, dict) and \"name\" in item:\n",
    "                tool_calls.append(item[\"name\"])\n",
    "    \n",
    "    return tool_calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Define the Target Function\n",
    "\n",
    "We stream the agent execution and collect all tool calls to build the trajectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent_trajectory(inputs: dict):\n",
    "    \"\"\"Run agent and track the trajectory of tool calls.\"\"\"\n",
    "    configuration = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "    trajectory = []\n",
    "    async for chunk in sentiment_agent.astream(\n",
    "        {\"messages\": [HumanMessage(content=inputs[\"query\"])]},\n",
    "        stream_mode=\"debug\",\n",
    "        config=configuration\n",
    "    ):\n",
    "        # Event type for entering a node\n",
    "        if chunk.get('type') == 'task':\n",
    "            if \"tool\" in chunk.get('payload', {}).get('name', ''):\n",
    "                input_data = chunk['payload']['input']\n",
    "                tools = extract_tool_calls(input_data)\n",
    "                trajectory.extend(tools)\n",
    "    \n",
    "    return {\"trajectory\": trajectory}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Define the Evaluator\n",
    "\n",
    "We'll use an exact match evaluator to check if the trajectory matches the expected sequence of tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_exact_match(outputs: dict, reference_outputs: dict):\n",
    "    \"\"\"Evaluate whether the trajectory exactly matches the expected output.\"\"\"\n",
    "    return {\n",
    "        \"key\": \"exact_match\", \n",
    "        \"score\": outputs[\"trajectory\"] == reference_outputs[\"trajectory\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Run the Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    run_agent_trajectory,\n",
    "    data=trajectory_dataset_name,\n",
    "    evaluators=[evaluate_exact_match],\n",
    "    experiment_prefix=\"clinical-trajectory\",\n",
    "    max_concurrency=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluating the ML Model (Precision, Recall, F1, Accuracy)\n",
    "\n",
    "Finally, let's evaluate the ML model itself as a tool. This is important because:\n",
    "- It establishes a baseline for model performance\n",
    "- It helps identify if issues come from the ML model vs. the agent logic\n",
    "- These metrics (precision, recall, F1) are standard for classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create the Dataset\n",
    "\n",
    "We'll create a dataset with labeled sentiment examples. In production, you'd use a curated test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with labeled clinical trial participant feedback\n",
    "ml_examples = [\n",
    "    # Positive feedback - treatment efficacy and satisfaction\n",
    "    {\"text\": \"The treatment has significantly improved my quality of life. I can finally sleep through the night.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"My symptoms have decreased by at least 80% since starting the trial medication.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"I'm thrilled with the results. The study coordinators were professional and supportive throughout.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"No significant side effects and noticeable improvement within the first two weeks.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"This is the first treatment that has actually worked for my condition. I'm grateful I enrolled.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"The medication was easy to take and the monitoring visits were well-organized.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"I experienced remarkable improvement in my mobility and reduced pain levels.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Excellent care from the research team and promising results from the treatment.\", \"label\": \"POSITIVE\"},\n",
    "    \n",
    "    # Negative feedback - side effects and concerns\n",
    "    {\"text\": \"I had to discontinue due to severe nausea and persistent headaches.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"No noticeable improvement after 8 weeks. Very disappointed with the results.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"The side effects were unbearable - constant fatigue and dizziness.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"I experienced adverse reactions that significantly impacted my daily activities.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"The treatment made my symptoms worse. I had to withdraw from the study.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Poor communication from the study team and long wait times at every visit.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Severe insomnia and mood changes since starting the medication.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"I regret participating. The benefits did not outweigh the side effects.\", \"label\": \"NEGATIVE\"},\n",
    "]\n",
    "\n",
    "dataset_name = \"ML Model Evals: Clinical Trial Sentiment\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"text\": ex[\"text\"]} for ex in ml_examples],\n",
    "        outputs=[{\"label\": ex[\"label\"]} for ex in ml_examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    print(f\"Created dataset: {dataset_name}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Define the Target Function\n",
    "\n",
    "For ML model evaluation, we run the tool directly (not the full agent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_model(inputs: dict) -> dict:\n",
    "    \"\"\"Run the sentiment model directly on input text.\"\"\"\n",
    "    result = sentiment_pipeline(inputs[\"text\"])[0]\n",
    "    return {\"label\": result[\"label\"], \"confidence\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Define ML Metric Evaluators\n",
    "\n",
    "We'll create evaluators for standard classification metrics. You may want to compute the overall pass rate or F1 score across all examples in the dataset—these are called **summary evaluators**, while per-row scores use **row-level evaluators**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(runs, examples):\n",
    "    \"\"\"Summary evaluator that computes precision, recall, F1 across all examples.\"\"\"\n",
    "    tp = fp = tn = fn = 0\n",
    "    \n",
    "    for run, example in zip(runs, examples):\n",
    "        predicted = run.outputs.get(\"label\")\n",
    "        expected = example.outputs.get(\"label\")\n",
    "        \n",
    "        if predicted == \"POSITIVE\" and expected == \"POSITIVE\":\n",
    "            tp += 1\n",
    "        elif predicted == \"POSITIVE\" and expected == \"NEGATIVE\":\n",
    "            fp += 1\n",
    "        elif predicted == \"NEGATIVE\" and expected == \"NEGATIVE\":\n",
    "            tn += 1\n",
    "        else:  # predicted NEGATIVE, expected POSITIVE\n",
    "            fn += 1\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\"key\": \"precision\", \"score\": precision},\n",
    "            {\"key\": \"recall\", \"score\": recall},\n",
    "            {\"key\": \"f1_score\", \"score\": f1},\n",
    "            {\"key\": \"accuracy\", \"score\": accuracy},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def correct(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Row-level correctness evaluator.\"\"\"\n",
    "    return outputs[\"label\"] == reference_outputs[\"label\"]\n",
    "\n",
    "def calibration(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Row-level calibration error - difference between confidence and correctness.\"\"\"\n",
    "    is_correct = 1 if outputs[\"label\"] == reference_outputs[\"label\"] else 0\n",
    "    confidence = outputs[\"confidence\"]\n",
    "    \n",
    "    # Calibration error: how far off was the confidence from reality?\n",
    "    # Perfect calibration = 0 (e.g., 90% confident and correct, or 10% confident and wrong)\n",
    "    calibration_error = abs(confidence - is_correct)\n",
    "    \n",
    "    return {\"key\": \"calibration_error\", \"score\": calibration_error}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Run the Evaluation\n",
    "\n",
    "We'll evaluate the model across 4 different token lengths for the DistilBERT sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_to_test = [64, 128, 256, 512]\n",
    "\n",
    "for max_len in max_lengths_to_test:\n",
    "    pipe = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "    \n",
    "    def run_model(inputs: dict) -> dict:\n",
    "        result = pipe(inputs[\"text\"])[0]\n",
    "        return {\"label\": result[\"label\"], \"confidence\": result[\"score\"]}\n",
    "    \n",
    "    client.evaluate(\n",
    "        run_model,\n",
    "        data=dataset_name,\n",
    "        evaluators=[correct, calibration],\n",
    "        summary_evaluators=[classification_metrics],\n",
    "        experiment_prefix=f\"sentiment-maxlen-{max_len}\",\n",
    "        metadata={\"max_length\": max_len}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
