{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating ML Models as Agent Tools\n",
    "\n",
    "In this notebook, we'll demonstrate how to evaluate traditional ML models when used as tools within LangGraph agents. We'll build a simple sentiment analysis agent and show multiple evaluation approaches:\n",
    "\n",
    "1. **ML Model Metrics** - Precision, Recall, F1, Accuracy, Calibration on the model itself. Discussions on evaluating hyperaparameters.\n",
    "2. **Single Step Evaluation** - Did the agent correctly select the right tool?\n",
    "3. **End-to-End Evaluation** - Does the agent produce correct final responses?\n",
    "\n",
    "This pattern applies to any ML model or tool called by an agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations: Key Concepts\n",
    "\n",
    "**Evaluations** are a quantitative way to measure performance of agents, which is important because LLMs don't always behave predictably — small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\n",
    "\n",
    "Evaluations are made up of three components:\n",
    "\n",
    "1. A **dataset** with test inputs and expected outputs.\n",
    "2. An **application or target function** that defines what you are evaluating, taking in inputs and returning the application output\n",
    "3. **Evaluators** that score your target function's outputs.\n",
    "\n",
    "![Evaluation](images/evals-conceptual.png)\n",
    "\n",
    "In this notebook, we'll cover:\n",
    "\n",
    "1. **ML Model Metrics**: Evaluate the ML tool directly with precision, recall, F1, accuracy\n",
    "2. **Single Step**: Evaluate whether the agent selects the appropriate tool\n",
    "3. **Final Response**: Evaluate the agent's end-to-end output quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "First, let's install the required packages. We need:\n",
    "- `transformers` and `torch` for the HuggingFace sentiment model\n",
    "- `langchain` and `langgraph` for building the agent\n",
    "- `langsmith` and `openevals` for evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "**Use Case: Clinical Trial Participant Feedback Analysis**\n",
    "\n",
    "In pharmaceutical research, analyzing sentiment from clinical trial participant feedback helps identify:\n",
    "- Treatment satisfaction and efficacy perceptions\n",
    "- Side effect severity and patient concerns  \n",
    "- Early signals of safety issues\n",
    "- Overall participant experience\n",
    "\n",
    "We'll build an agent that uses sentiment analysis to classify this feedback, then evaluate it thoroughly.\n",
    "\n",
    "Make sure you have the following environment variables set:\n",
    "- `LANGSMITH_API_KEY` - For LangSmith evaluations\n",
    "- `OPENAI_API_KEY` - For the LLM powering the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the LLM for our agent\n",
    "model = init_chat_model(\"openai:gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building the Sentiment Analysis Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create the Sentiment Analysis Tool\n",
    "\n",
    "We'll wrap a HuggingFace sentiment analysis model as a LangChain tool. This model (`distilbert-base-uncased-finetuned-sst-2-english`) classifies text as POSITIVE or NEGATIVE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sentiment tool:\n",
      "{'label': 'POSITIVE', 'score': 0.9999}\n",
      "{'label': 'NEGATIVE', 'score': 0.9996}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Initialize the HuggingFace sentiment pipeline\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "@tool\n",
    "def analyze_sentiment(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of the given text.\n",
    "    Returns the sentiment label (POSITIVE/NEGATIVE) and confidence score.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze for sentiment\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'label' (POSITIVE/NEGATIVE) and 'score' (confidence 0-1)\n",
    "    \"\"\"\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return {\"label\": result[\"label\"], \"score\": round(result[\"score\"], 4)}\n",
    "\n",
    "# Test the tool\n",
    "print(\"Testing sentiment tool:\")\n",
    "print(analyze_sentiment.invoke(\"I love this product, it's amazing!\"))\n",
    "print(analyze_sentiment.invoke(\"This is terrible, I hate it.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create the Agent\n",
    "\n",
    "Now let's create a simple agent using LangChain's `create_agent()` that can use our sentiment analysis tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x1374b7c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# System prompt for our clinical trial feedback sentiment agent\n",
    "sentiment_agent_prompt = \"\"\"\n",
    "You are a clinical trial feedback analyst that helps pharmaceutical researchers \n",
    "understand participant sentiment.\n",
    "\n",
    "When given participant feedback, use the analyze_sentiment tool to classify it \n",
    "as positive or negative. After analysis, provide context on what the sentiment \n",
    "might indicate:\n",
    "- Positive feedback often suggests treatment efficacy and good tolerability\n",
    "- Negative feedback may signal side effects, lack of efficacy, or participant concerns\n",
    "\n",
    "If the user asks something unrelated to clinical trial feedback analysis, politely \n",
    "explain that you specialize in analyzing participant sentiment from clinical studies.\n",
    "\"\"\"\n",
    "\n",
    "# Create checkpointer for conversation memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Create the agent\n",
    "sentiment_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[analyze_sentiment],\n",
    "    name=\"clinical_feedback_agent\",\n",
    "    system_prompt=sentiment_agent_prompt,\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "sentiment_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Test the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Analyze this participant feedback: 'This treatment has been life-changing! My symptoms have completely disappeared and I feel better than I have in years. I'm so grateful I enrolled in this trial.'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  analyze_sentiment (call_Q8PUyiqSdJFtIszAW4EqLT4y)\n",
      " Call ID: call_Q8PUyiqSdJFtIszAW4EqLT4y\n",
      "  Args:\n",
      "    text: This treatment has been life-changing! My symptoms have completely disappeared and I feel better than I have in years. I'm so grateful I enrolled in this trial.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: analyze_sentiment\n",
      "\n",
      "{\"label\": \"POSITIVE\", \"score\": 0.9962}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sentiment analysis indicates a **positive** sentiment with a high confidence score of 0.9962. \n",
      "\n",
      "This feedback suggests that the participant has experienced significant treatment efficacy, as their symptoms have completely disappeared. Additionally, the expression of gratitude for enrolling in the trial indicates good tolerability and overall satisfaction with the treatment experience.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langsmith import uuid7\n",
    "\n",
    "# Test with clinical trial participant feedback\n",
    "config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "result = sentiment_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Analyze this participant feedback: 'This treatment has been life-changing! My symptoms have completely disappeared and I feel better than I have in years. I'm so grateful I enrolled in this trial.'\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the recommended dosage for ibuprofen?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I specialize in analyzing participant sentiment from clinical studies and not in providing medical dosages or recommendations. For dosage information regarding ibuprofen, I recommend consulting a healthcare professional or the official product instructions.\n"
     ]
    }
   ],
   "source": [
    "# Test with an unrelated query (should politely decline)\n",
    "config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "result = sentiment_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the recommended dosage for ibuprofen?\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluating the ML Model (Precision, Recall, F1, Accuracy)\n",
    "\n",
    "Before evaluating the full agent, let's evaluate the ML model itself. This is important because:\n",
    "- It establishes a baseline for model performance\n",
    "- It helps identify if issues come from the ML model vs. the agent logic\n",
    "- These metrics (precision, recall, F1) are standard for classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create the Dataset\n",
    "\n",
    "We'll create a dataset with labeled sentiment examples. In production, you'd use a curated test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists: ML Model Evals: Clinical Trial Sentiment\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Dataset with labeled clinical trial participant feedback\n",
    "ml_examples = [\n",
    "    # Positive feedback - treatment efficacy and satisfaction\n",
    "    {\"text\": \"The treatment has significantly improved my quality of life. I can finally sleep through the night.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"My symptoms have decreased by at least 80% since starting the trial medication.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"I'm thrilled with the results. The study coordinators were professional and supportive throughout.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"No significant side effects and noticeable improvement within the first two weeks.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"This is the first treatment that has actually worked for my condition. I'm grateful I enrolled.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"The medication was easy to take and the monitoring visits were well-organized.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"I experienced remarkable improvement in my mobility and reduced pain levels.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"Excellent care from the research team and promising results from the treatment.\", \"label\": \"POSITIVE\"},\n",
    "    \n",
    "    # Negative feedback - side effects and concerns\n",
    "    {\"text\": \"I had to discontinue due to severe nausea and persistent headaches.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"No noticeable improvement after 8 weeks. Very disappointed with the results.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"The side effects were unbearable - constant fatigue and dizziness.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"I experienced adverse reactions that significantly impacted my daily activities.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"The treatment made my symptoms worse. I had to withdraw from the study.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Poor communication from the study team and long wait times at every visit.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Severe insomnia and mood changes since starting the medication.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"I regret participating. The benefits did not outweigh the side effects.\", \"label\": \"NEGATIVE\"},\n",
    "]\n",
    "\n",
    "dataset_name = \"ML Model Evals: Clinical Trial Sentiment\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"text\": ex[\"text\"]} for ex in ml_examples],\n",
    "        outputs=[{\"label\": ex[\"label\"]} for ex in ml_examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    print(f\"Created dataset: {dataset_name}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define the Target Function\n",
    "\n",
    "For ML model evaluation, we run the tool directly (not the full agent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_model(inputs: dict) -> dict:\n",
    "    \"\"\"Run the sentiment model directly on input text.\"\"\"\n",
    "    result = sentiment_pipeline(inputs[\"text\"])[0]\n",
    "    return {\"label\": result[\"label\"], \"score\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define ML Metric Evaluators\n",
    "\n",
    "We'll create an evaluators for standard classification metrics. You may want to compute the overall pass rate or F1 score across all examples in the dataset—these are called **summary evaluators**, while per-row scores use **row-level evaluators**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(runs, examples):\n",
    "    \"\"\"Summary evaluator that computes precision, recall, F1 across all examples.\"\"\"\n",
    "    tp = fp = tn = fn = 0\n",
    "    \n",
    "    for run, example in zip(runs, examples):\n",
    "        predicted = run.outputs.get(\"label\")\n",
    "        expected = example.outputs.get(\"label\")\n",
    "        \n",
    "        if predicted == \"POSITIVE\" and expected == \"POSITIVE\":\n",
    "            tp += 1\n",
    "        elif predicted == \"POSITIVE\" and expected == \"NEGATIVE\":\n",
    "            fp += 1\n",
    "        elif predicted == \"NEGATIVE\" and expected == \"NEGATIVE\":\n",
    "            tn += 1\n",
    "        else:  # predicted NEGATIVE, expected POSITIVE\n",
    "            fn += 1\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\"key\": \"precision\", \"score\": precision},\n",
    "            {\"key\": \"recall\", \"score\": recall},\n",
    "            {\"key\": \"f1_score\", \"score\": f1},\n",
    "            {\"key\": \"accuracy\", \"score\": accuracy},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def correct(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Row-level correctness evaluator.\"\"\"\n",
    "    return outputs[\"label\"] == reference_outputs[\"label\"]\n",
    "\n",
    "def calibration(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Row-level calibration evaluator - tracks confidence alongside correctness.\"\"\"\n",
    "    is_correct = outputs[\"label\"] == reference_outputs[\"label\"]\n",
    "    confidence = outputs[\"score\"]\n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\"key\": \"confidence\", \"score\": confidence},\n",
    "            {\"key\": \"correct\", \"score\": 1 if is_correct else 0},\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Run the Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sentiment-maxlen-64-5c9dbafc' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/ef87e6ea-e18e-42eb-b51e-ff7bb1971d21/compare?selectedSessions=77eab534-d922-4123-ae18-8a9a255c5a81\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2019c118b8d4a87b5d41d189f462aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sentiment-maxlen-128-1b479aec' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/ef87e6ea-e18e-42eb-b51e-ff7bb1971d21/compare?selectedSessions=1769983b-4832-46a5-af02-7db656602053\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf94a2b0edc6462493508a1f1113bfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sentiment-maxlen-256-1e194422' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/ef87e6ea-e18e-42eb-b51e-ff7bb1971d21/compare?selectedSessions=8d6abb4a-1a5f-4f70-843f-31d9ae4a8855\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc53c9348f145fdb8434bbe7f2e6bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sentiment-maxlen-512-0dd2d87a' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/ef87e6ea-e18e-42eb-b51e-ff7bb1971d21/compare?selectedSessions=56409036-807f-4a20-8ae6-3951ea8d43d0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b0a438837a4845893f20523796153a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_lengths_to_test = [64, 128, 256, 512]\n",
    "\n",
    "for max_len in max_lengths_to_test:\n",
    "    pipe = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )\n",
    "    \n",
    "    def run_model(inputs: dict) -> dict:\n",
    "        result = pipe(inputs[\"text\"])[0]\n",
    "        return {\"label\": result[\"label\"], \"score\": result[\"score\"]}\n",
    "    \n",
    "    client.evaluate(\n",
    "        run_model,\n",
    "        data=dataset_name,\n",
    "        evaluators=[correct, calibration],\n",
    "        summary_evaluators=[classification_metrics],\n",
    "        experiment_prefix=f\"sentiment-maxlen-{max_len}\",\n",
    "        metadata={\"max_length\": max_len}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results in LangSmith! You should see accuracy, precision, recall, confidence, and calibration metrics aggregated across all examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Online vs Offline Evaluations\n",
    "\n",
    "Before diving into agent-level evaluations, let's discuss two important evaluation paradigms:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Evaluations (What We've Done So Far)\n",
    "\n",
    "**Offline evaluations** run before deployment on curated test datasets:\n",
    "\n",
    "- Use `client.evaluate()` or `client.aevaluate()` \n",
    "- Run on a fixed dataset with known expected outputs\n",
    "- Great for:\n",
    "  - **Regression testing** - Ensure changes don't break existing functionality\n",
    "  - **Comparing model versions** - A/B test different prompts or models\n",
    "  - **CI/CD pipelines** - Automated quality gates before deployment\n",
    "- Results visible in LangSmith **Experiments** view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Evaluations (Production Monitoring)\n",
    "\n",
    "**Online evaluations** run on real production traffic:\n",
    "\n",
    "- Set up via **LangSmith Automations/Rules** in the UI\n",
    "- **Sample-based** - Evaluate a percentage of production runs (e.g., 10%)\n",
    "- Can use LLM-as-judge on live data\n",
    "- Enables:\n",
    "  - **Real-time monitoring** - Dashboards for production quality\n",
    "  - **Alerting** - Get notified when quality drops\n",
    "  - **Drift detection** - Catch issues with real user inputs\n",
    "- Results visible in LangSmith **Monitoring** view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each\n",
    "\n",
    "| Aspect | Offline | Online |\n",
    "|--------|---------|--------|\n",
    "| When | Before deployment | After deployment |\n",
    "| Data | Curated test sets | Real user traffic |\n",
    "| Coverage | 100% of test set | Sampled % of traffic |\n",
    "| Purpose | Validate changes | Monitor production |\n",
    "| Speed | Fast feedback | Continuous |\n",
    "\n",
    "**Best practice**: Use both! Offline evals for development, online evals for production monitoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Single Step Evaluation\n",
    "\n",
    "Now let's evaluate whether the agent correctly decides when to use the sentiment tool.\n",
    "\n",
    "- Input: User query\n",
    "- Output: Whether the correct tool was selected (or no tool for off-topic queries)\n",
    "\n",
    "![single-step](../langgraph-101/images/single-step.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists: ML Model Evals: Clinical Trial Single Step\n"
     ]
    }
   ],
   "source": [
    "single_step_examples = [\n",
    "    # Should call sentiment tool - clinical trial feedback\n",
    "    {\"query\": \"Analyze this feedback: 'The medication greatly improved my symptoms'\", \"should_call_tool\": True},\n",
    "    {\"query\": \"What's the sentiment of: 'I experienced severe nausea and had to withdraw'\", \"should_call_tool\": True},\n",
    "    {\"query\": \"Is this participant feedback positive or negative: 'No side effects and feeling much better'\", \"should_call_tool\": True},\n",
    "    {\"query\": \"Classify the sentiment: 'The treatment was ineffective and I saw no improvement'\", \"should_call_tool\": True},\n",
    "    {\"query\": \"How does this participant feel: 'I'm grateful I enrolled, life-changing results'\", \"should_call_tool\": True},\n",
    "    \n",
    "    # Should NOT call sentiment tool - off-topic questions\n",
    "    {\"query\": \"What's the recommended dosage for the study medication?\", \"should_call_tool\": False},\n",
    "    {\"query\": \"When is my next clinic visit scheduled?\", \"should_call_tool\": False},\n",
    "    {\"query\": \"Can I take ibuprofen while on the trial?\", \"should_call_tool\": False},\n",
    "    {\"query\": \"What are the inclusion criteria for the study?\", \"should_call_tool\": False},\n",
    "    {\"query\": \"Who is the principal investigator?\", \"should_call_tool\": False},\n",
    "]\n",
    "\n",
    "single_step_dataset_name = \"ML Model Evals: Clinical Trial Single Step\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=single_step_dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=single_step_dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"query\": ex[\"query\"]} for ex in single_step_examples],\n",
    "        outputs=[{\"should_call_tool\": ex[\"should_call_tool\"]} for ex in single_step_examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    print(f\"Created dataset: {single_step_dataset_name}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {single_step_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define the Target Function\n",
    "\n",
    "We'll run the agent and check if it called the sentiment tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent_single_step(inputs: dict) -> dict:\n",
    "    \"\"\"Run agent and check if sentiment tool was called.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "    \n",
    "    result = await sentiment_agent.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=inputs[\"query\"])]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Check if analyze_sentiment tool was called\n",
    "    tool_called = False\n",
    "    for message in result[\"messages\"]:\n",
    "        if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "            for tc in message.tool_calls:\n",
    "                if tc.get(\"name\") == \"analyze_sentiment\":\n",
    "                    tool_called = True\n",
    "                    break\n",
    "    \n",
    "    return {\"tool_called\": tool_called}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Define the Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_selection_evaluator(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Check if the agent correctly decided whether to call the tool.\"\"\"\n",
    "    expected = reference_outputs[\"should_call_tool\"]\n",
    "    actual = outputs[\"tool_called\"]\n",
    "    is_correct = expected == actual\n",
    "    return {\"key\": \"correct_tool_selection\", \"score\": 1 if is_correct else 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Run the Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sentiment-single-step-a075ff9f' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/e00f955a-57a1-4c58-a99c-8dacf1432a63/compare?selectedSessions=7f56a43b-8b63-4ff9-90bb-c10404b4e790\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7653f35af96b4935a06e12940d2a9da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    run_agent_single_step,\n",
    "    data=single_step_dataset_name,\n",
    "    evaluators=[tool_selection_evaluator],\n",
    "    experiment_prefix=\"sentiment-single-step\",\n",
    "    max_concurrency=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: End-to-End Evaluation (Final Response)\n",
    "\n",
    "Finally, let's evaluate the agent's complete responses. This treats the agent as a black box and evaluates whether it produces correct, helpful responses.\n",
    "\n",
    "- Input: User query\n",
    "- Output: Agent's final response\n",
    "\n",
    "![final-response](../langgraph-101/images/final-response.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists: ML Model Evals: Clinical Trial E2E v2\n"
     ]
    }
   ],
   "source": [
    "e2e_examples = [\n",
    "    # Positive feedback - efficacy\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'The treatment has dramatically reduced my symptoms. I finally feel like myself again.'\",\n",
    "        \"expected_response\": \"Positive sentiment indicating strong treatment efficacy and improved quality of life.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'No side effects and my pain levels have dropped significantly since week 2.'\",\n",
    "        \"expected_response\": \"Positive sentiment indicating good tolerability and measurable improvement.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'The study team was excellent and the medication worked better than anything I've tried before.'\",\n",
    "        \"expected_response\": \"Positive sentiment about both treatment efficacy and study experience.\"\n",
    "    },\n",
    "    \n",
    "    # Negative feedback - side effects\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'I had to withdraw due to constant nausea and severe headaches.'\",\n",
    "        \"expected_response\": \"Negative sentiment indicating tolerability issues leading to discontinuation.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'No improvement after 8 weeks. Very disappointed with the results.'\",\n",
    "        \"expected_response\": \"Negative sentiment indicating lack of efficacy.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'The side effects were worse than my original symptoms. I regret enrolling.'\",\n",
    "        \"expected_response\": \"Negative sentiment indicating poor risk-benefit experience.\"\n",
    "    },\n",
    "    \n",
    "    # Mixed/nuanced feedback\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'The medication helped my condition but the fatigue was hard to manage.'\",\n",
    "        \"expected_response\": \"Mixed but net positive - efficacy achieved with manageable side effects.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Analyze this feedback: 'Some improvement but not sure if it's worth continuing given the stomach issues.'\",\n",
    "        \"expected_response\": \"Uncertain/negative sentiment - questioning whether to continue.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "e2e_dataset_name = \"ML Model Evals: Clinical Trial E2E v2\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=e2e_dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=e2e_dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"query\": ex[\"query\"]} for ex in e2e_examples],\n",
    "        outputs=[{\"response\": ex[\"response\"]} for ex in e2e_examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    print(f\"Created dataset: {e2e_dataset_name}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists: {e2e_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Define the Target Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "async def run_agent_e2e(inputs: dict) -> dict:\n",
    "    \"\"\"Run the full agent and return the final response.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "    \n",
    "    result = await sentiment_agent.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=inputs[\"query\"])]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Just return the final response\n",
    "    return {\"response\": result[\"messages\"][-1].content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Define the Evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openevals.llm import create_async_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "\n",
    "# LLM-as-judge for response correctness\n",
    "correctness_evaluator = create_async_llm_as_judge(\n",
    "    prompt=CORRECTNESS_PROMPT,\n",
    "    feedback_key=\"correctness\",\n",
    "    judge=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Run the Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sentiment-e2e-5c81c90c' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/25dac4d7-e499-4e2a-813a-520d86bdcbea/compare?selectedSessions=1901c773-501d-49aa-acd7-81d608f5b1dd\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0953947aa53467c87ce5c14c81c01c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    run_agent_e2e,\n",
    "    data=e2e_dataset_name,\n",
    "    evaluators=[correctness_evaluator],\n",
    "    experiment_prefix=\"sentiment-e2e\",\n",
    "    max_concurrency=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated a comprehensive evaluation strategy for ML models used as agent tools:\n",
    "\n",
    "1. **ML Model Metrics** - Evaluated the sentiment classifier directly with precision, recall, accuracy, and confidence scores\n",
    "\n",
    "2. **Online vs Offline Evals** - Discussed when to use offline evaluations (development) vs online evaluations (production monitoring)\n",
    "\n",
    "3. **Single Step Evaluation** - Verified the agent correctly decides when to call the sentiment tool\n",
    "\n",
    "4. **End-to-End Evaluation** - Assessed the agent's complete responses using LLM-as-judge and custom evaluators\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Layer your evaluations**: Test the ML model, the tool selection logic, and the full agent separately\n",
    "- **Use appropriate metrics**: Classification metrics for ML models, correctness for agent responses\n",
    "- **Combine evaluators**: Use both deterministic checks and LLM-as-judge for comprehensive coverage\n",
    "- **View results in LangSmith**: All evaluation results are tracked and visualized in the Experiments view\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Set up **online evaluations** in LangSmith for production monitoring\n",
    "- Add more sophisticated ML models (toxicity, entity extraction, etc.)\n",
    "- Create **regression test suites** that run in CI/CD\n",
    "- Experiment with different prompts and compare results across experiments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
